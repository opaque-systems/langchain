import logging
from typing import Any, Dict, List, Optional

from pydantic import Extra, root_validator

from langchain.callbacks.manager import CallbackManagerForLLMRun
from langchain.llms.base import LLM
from langchain.prompts.base import StringPromptValue
from langchain.utils import get_from_dict_or_env

logger = logging.getLogger(__name__)


class PromptGuardLLMWrapper(LLM):
    """
    A LLM wrapper that uses the PromptGuard to sanitize the prompt before
    passing it to the LLM, and desanitize the response after
    getting it from the LLM.

    To use, you should have the `promptguard` python package installed,
    and the environment variable `PROMPTGUARD_API_KEY` set with
    your access token, or pass it as a named parameter to the constructor.

    Example:
        .. code-block:: python

            prompt_guard_llm = PromptGuardLLM(llm=ChatOpenAI())
    """

    llm: Any
    """The LLM to use."""

    class Config:
        """Configuration for this pydantic object."""

        extra = Extra.forbid

    @root_validator()
    def validate_environment(cls, values: Dict) -> Dict:
        """Validates that the PromptGuard API key and the Python package exist."""
        token = get_from_dict_or_env(
            values, "promptguard_api_key", "PROMPTGUARD_API_KEY"
        )
        if token is None:
            raise ValueError(
                "Could not find PROMPTGUARD_API_KEY in the environment. "
                "Please set it to your PromptGuard API key."
                "You can get one at <UI> after accepted from the waitlist."
            )
        try:
            import promptguard as pg

            assert pg.__package__ is not None
        except ImportError:
            raise ImportError(
                "Could not import the `promptguard` python package. "
                "Please install it with `pip install promptguard`."
            )
        return values

    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        """Call out to PromptGuard to do sanitization and desanitization
        before and after running LLM.

        This is an override of the base class method.

        Args:
            prompt: The prompt to pass into the model.
        Returns:
            The string generated by the model.
        Example:
            .. code-block:: python
                response = prompt_guard_llm("Tell me a joke.")
        """
        import promptguard as pg

        # sanitize the prompt, by replacing the sensitive information with a placeholder
        sanitize_response: pg.SanitizeResponse = pg.sanitize(prompt)
        sanitized_prompt_value_str = sanitize_response.sanitized_text

        # call the llm with the sanitized prompt and get the response
        llm_response = self.llm.generate_prompt(
            [StringPromptValue(text=sanitized_prompt_value_str)],
        )

        # desanitize the response by restoring the original sensitive information
        desanitize_response: pg.DesanitizeResponse = pg.desanitize(
            llm_response.generations[0][0].text,
            secure_context=sanitize_response.secure_context,
        )
        return desanitize_response.desanitized_text

    @property
    def _llm_type(self) -> str:
        """Return type of llm.

        This is an override of the base class method.
        """
        return "promptguard"
