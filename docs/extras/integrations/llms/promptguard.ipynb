{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PromptGuard\n",
    "\n",
    "[PromptGuard](https://github.com/opaque-systems/promptguard-python) preserves the privacy in your prompts.\n",
    " \n",
    "\n",
    "This notebook goes over how to use LangChain to interact with `PromptGuard`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the promptguard and langchain packages\n",
    "! pip install promptguard langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "# Set API keys\n",
    "# Join the [waitlist](tbd) for the an API Key to access the promptguard API.\n",
    "\n",
    "os.environ['PROMPT_GUARD_ACCESS_TOKEN'] = \"<PROMPT_GUARD_API_KEY>\"\n",
    "os.environ['OPENAI_API_KEY'] = \"<OPENAI_API_KEY>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use PromptGuardLLMWrapper\n",
    "\n",
    "Applying promptguard to your application could be as simple as wrapping your LLM using the PromptGuardLLMWrapper class. \n",
    "```python\n",
    "# llm=OpenAI()\n",
    "llm=PromptGuardLLMWrapper(OpenAI())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain import LLMChain, PromptTemplate\n",
    "from langchain.callbacks.stdout import StdOutCallbackHandler\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "from langchain.llms import PromptGuardLLMWrapper\n",
    "\n",
    "langchain.verbose = True\n",
    "langchain.debug = True\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "As an AI assisant, you will answer questions according to given context.\n",
    "\n",
    "Important PII data is sanitized in the question.\n",
    "For example, \"Giana is good\" is sanitized to \"PERSON_999 is good\".\n",
    "You must treat the sanitized data as opaque strings, but you can use them as\n",
    "meaningful entities in the response.\n",
    "Different sanitized items could be the same entity based on the semantics.\n",
    "You must keep the sanitized item as is and cannot change it.\n",
    "The format of sanitized item is \"TYPE_ID\".\n",
    "You must not create new sanitized items following the format. For example, you\n",
    "cannot create \"PERSON_1000\" or \"PERSON_998\" if \"PERSON_1000\" or \"PERSON_998\" is\n",
    "not in the question.\n",
    "\n",
    "Conversation History: ```{history}```\n",
    "Context : ```Mr. Carl Smith is a 31-year-old man who has been experiencing\n",
    "homelessness on and off for all his adult life. Mr. Smith says he is about\n",
    "5’5” and weighs approximately 129 lbs. He presents as\n",
    "very thin, typically wearing a clean white undershirt\n",
    "and loose-fitting khaki shorts at interviews.\n",
    "His brown hair is disheveled and dirty looking, and\n",
    "he constantly fidgets and shakes his hand or\n",
    "knee during interviews. Despite his best efforts, Carl is a poor historian. ```\n",
    "Question: ```{question}```\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "chain = LLMChain(\n",
    "    prompt=PromptTemplate.from_template(prompt_template),\n",
    "    llm=PromptGuardLLMWrapper(llm=OpenAI()),\n",
    "    memory=ConversationBufferWindowMemory(k=2),\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "\n",
    "print(\n",
    "    chain.run(\n",
    "        {\"question\": \"\"\"How high is he? \"\"\"},\n",
    "        callbacks=[StdOutCallbackHandler()],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output, you can see the following context from user input has sensitive data.\n",
    "\n",
    "``` \n",
    "# Context from user input\n",
    "\n",
    "Mr. Carl Smith is a 31-year-old man who has been experiencing\n",
    "homelessness on and off for all his adult life. Mr. Smith says he is about\n",
    "5’5” and weighs approximately 129 lbs. He presents as\n",
    "very thin, typically wearing a clean white undershirt\n",
    "and loose-fitting khaki shorts at interviews.\n",
    "His brown hair is disheveled and dirty looking, and\n",
    "he constantly fidgets and shakes his hand or\n",
    "knee during interviews. Despite his best efforts, Carl is a poor historian. \n",
    "```\n",
    "\n",
    "PromptGuard will automatically detect the sensitive data and replace it with a placeholder. \n",
    "\n",
    "```\n",
    "\n",
    "# Context after PromptGuard\n",
    "\n",
    "Mr. PERSON_5 is a DATE_TIME_1 man who has been experiencing\n",
    "homelessness on and off for all his adult life. Mr. PERSON_4 says he is about\n",
    "5’5” and weighs approximately 129 lbs. He presents as\n",
    "very thin, typically wearing a clean white undershirt\n",
    "and loose-fitting PERSON_3 at interviews.\n",
    "His brown hair is disheveled and dirty looking, and\n",
    "he constantly fidgets and shakes his hand or\n",
    "knee during interviews. Despite his best efforts, PERSON_2 is a poor historian.\n",
    "```\n",
    "\n",
    "Placeholder is used in the LLM response.\n",
    "\n",
    "```\n",
    "# response returned by LLM\n",
    "Mr. PERSON_5 is approximately 5'5\" tall.\n",
    "```\n",
    "\n",
    "Response is desanitized by replacing the placeholder with the original sensitive data.\n",
    "\n",
    "```\n",
    "# desanitized LLM response from PromptGuard\n",
    "Mr. Carl Smith is approximately 5'5\" tall.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use promptguard in LangChain expression\n",
    "\n",
    "There are functions that can be used with LangChain expression as well if a drop-in replacement doesn't offer the flexibility you need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain.utilities.promptguard as pgf\n",
    "from langchain.schema.runnable import RunnableMap\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "\n",
    "prompt=PromptTemplate.from_template(prompt_template),    \n",
    "llm = OpenAI()\n",
    "pg_chain = (\n",
    "    pgf.sanitize\n",
    "    | RunnableMap(\n",
    "        {\n",
    "            \"response\": (lambda x: x[\"sanitized_input\"])\n",
    "            | prompt\n",
    "            | llm\n",
    "            | StrOutputParser(),\n",
    "            \"secure_context\": lambda x: x[\"secure_context\"],\n",
    "        }\n",
    "    )\n",
    "    | (lambda x: pgf.desanitize(x[\"response\"], x[\"secure_context\"]))\n",
    ")\n",
    "\n",
    "pg_chain.invoke({\"question\": \"How high is he?\", \"history\": \"\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
